{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtoUQbNF86KE34iFHJ80oV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rekg/neuralnetwork/blob/main/neuralnetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used in this output is synthetically generated within the code for the purpose of testing the neural network implementation."
      ],
      "metadata": {
        "id": "RE0j8Rx5RMMT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtU9r7T82Fmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153b6482-6519-47a0-dcb9-3c441040269a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter input shape (number of features): 5\n",
            "Enter number of layers: 5\n",
            "Enter number of neurons for layer 1: 8\n",
            "Enter number of neurons for layer 2: 8\n",
            "Enter number of neurons for layer 3: 8\n",
            "Enter number of neurons for layer 4: 8\n",
            "Enter number of neurons for layer 5: 8\n",
            "Enter number of training epochs: 50\n",
            "Enter batch size: 32\n",
            "Enter number of output classes: 5\n",
            "Epoch 1/50, Loss: 0.0641\n",
            "Epoch 50/50, Loss: 0.0624\n",
            "Testing predictions:\n",
            "Predicted: [2 0 0 0 0 0 2 0 0 1]\n",
            "True Labels: [2 0 4 1 0 1 2 2 2 4]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(predictions, labels):\n",
        "    m = labels.shape[0]\n",
        "    p = softmax(predictions)\n",
        "    log_likelihood = -np.log(p[range(m), np.argmax(labels, axis=1)] + 1e-9)\n",
        "    return np.sum(log_likelihood) / m\n",
        "\n",
        "def one_hot(y, num_classes):\n",
        "    one_hot_encoded = np.zeros((len(y), num_classes))\n",
        "    one_hot_encoded[np.arange(len(y)), y] = 1\n",
        "    return one_hot_encoded\n",
        "\n",
        "def adam_optimizer(w, b, dw, db, lr, mw, vw, mb, vb, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    mw = beta1 * mw + (1 - beta1) * dw\n",
        "    vw = beta2 * vw + (1 - beta2) * (dw ** 2)\n",
        "    mw_corr = mw / (1 - beta1 ** t)\n",
        "    vw_corr = vw / (1 - beta2 ** t)\n",
        "    w -= lr * mw_corr / (np.sqrt(vw_corr) + epsilon)\n",
        "\n",
        "    mb = beta1 * mb + (1 - beta1) * db\n",
        "    vb = beta2 * vb + (1 - beta2) * (db ** 2)\n",
        "    mb_corr = mb / (1 - beta1 ** t)\n",
        "    vb_corr = vb / (1 - beta2 ** t)\n",
        "    b -= lr * mb_corr / (np.sqrt(vb_corr) + epsilon)\n",
        "\n",
        "    return w, b, mw, vw, mb, vb\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, layers, num_classes, activation='relu', optimizer='adam', learning_rate=0.01):\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.activation = relu\n",
        "        self.activation_derivative = relu_derivative\n",
        "        self.optimizer = optimizer\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.m_w, self.v_w = [], []\n",
        "        self.m_b, self.v_b = [], []\n",
        "        self.t = 1\n",
        "\n",
        "        layer_sizes = [input_size] + layers + [num_classes]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i])\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "            self.m_w.append(np.zeros_like(w))\n",
        "            self.v_w.append(np.zeros_like(w))\n",
        "            self.m_b.append(np.zeros_like(b))\n",
        "            self.v_b.append(np.zeros_like(b))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.layer_outputs = [X]\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            X = self.activation(np.dot(X, self.weights[i]) + self.biases[i])\n",
        "            self.layer_outputs.append(X)\n",
        "        output = np.dot(X, self.weights[-1]) + self.biases[-1]\n",
        "        self.layer_outputs.append(output)\n",
        "        return softmax(output)\n",
        "\n",
        "    def backward(self, X, y, y_pred):\n",
        "        m = X.shape[0]\n",
        "        gradients_w = []\n",
        "        gradients_b = []\n",
        "\n",
        "        # Output layer\n",
        "        dZ = y_pred - y\n",
        "        dW = np.dot(self.layer_outputs[-2].T, dZ) / m\n",
        "        dB = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        gradients_w.insert(0, dW)\n",
        "        gradients_b.insert(0, dB)\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in reversed(range(len(self.layers))):  # from last hidden layer back\n",
        "            dA = np.dot(dZ, self.weights[i + 1].T)\n",
        "            dZ = dA * self.activation_derivative(self.layer_outputs[i + 1])\n",
        "            dW = np.dot(self.layer_outputs[i].T, dZ) / m\n",
        "            dB = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "            gradients_w.insert(0, dW)\n",
        "            gradients_b.insert(0, dB)\n",
        "\n",
        "        return gradients_w, gradients_b\n",
        "\n",
        "    def train(self, X, y, epochs, batch_size=32):\n",
        "        n_samples = X.shape[0]\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle the dataset at the start of each epoch\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            loss_epoch = 0\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                y_pred = self.forward(X_batch)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = cross_entropy_loss(y_pred, y_batch)\n",
        "                loss_epoch += loss\n",
        "\n",
        "                # Backward pass and update weights\n",
        "                gradients_w, gradients_b = self.backward(X_batch, y_batch, y_pred)\n",
        "                for j in range(len(self.weights)):\n",
        "                    self.weights[j], self.biases[j], self.m_w[j], self.v_w[j], self.m_b[j], self.v_b[j] = adam_optimizer(\n",
        "                        self.weights[j], self.biases[j], gradients_w[j], gradients_b[j], self.learning_rate,\n",
        "                        self.m_w[j], self.v_w[j], self.m_b[j], self.v_b[j], self.t\n",
        "                    )\n",
        "\n",
        "                self.t += 1\n",
        "\n",
        "            if epoch % 100 == 0 or epoch == epochs - 1:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss_epoch / n_samples:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "# --- Input from user ---\n",
        "input_size = int(input(\"Enter input shape (number of features): \"))\n",
        "num_layers = int(input(\"Enter number of layers: \"))\n",
        "layers = [int(input(f\"Enter number of neurons for layer {i+1}: \")) for i in range(num_layers)]\n",
        "epochs = int(input(\"Enter number of training epochs: \"))\n",
        "batch_size = int(input(\"Enter batch size: \"))\n",
        "num_classes = int(input(\"Enter number of output classes: \"))\n",
        "\n",
        "# --- Generate dynamic training data matching input and output shapes ---\n",
        "n_samples = 100\n",
        "X_train = np.random.rand(n_samples, input_size)\n",
        "y_train_raw = np.random.randint(0, num_classes, size=n_samples)\n",
        "y_train = one_hot(y_train_raw, num_classes)\n",
        "\n",
        "# --- Initialize and train ---\n",
        "nn = NeuralNetwork(input_size=input_size, layers=layers, num_classes=num_classes, activation='relu', optimizer='adam', learning_rate=0.01)\n",
        "nn.train(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# --- Predictions ---\n",
        "print(\"Testing predictions:\")\n",
        "preds = nn.predict(X_train[:10])\n",
        "print(\"Predicted:\", preds)\n",
        "print(\"True Labels:\", y_train_raw[:10])\n"
      ]
    }
  ]
}